{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPK7++exVQ7jhXGl9k6SDzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abidlifiras/QA_LLM/blob/master/google_flan_t5_base_first_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abidlifiras/QA_LLM.git"
      ],
      "metadata": {
        "id": "LlA6Hb1LgwzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.17 datasets evaluate --quiet"
      ],
      "metadata": {
        "id": "nOazXtxpg1eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "NGUcPP1BmK3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "M30XUSV4g3cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "df_train = pd.read_json('QA_LLM/dataset/train.json')\n",
        "df_dev = pd.read_json('QA_LLM/dataset/dev.json')\n",
        "df_test = pd.read_json('QA_LLM/dataset/test.json')"
      ],
      "metadata": {
        "id": "FQCej4irhBxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIqY_OFZf_Y2"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "def format_prompt(question, answers_dict):\n",
        "    letters = ['a', 'b', 'c', 'd', 'e']\n",
        "    choices = [f\"{l}) {answers_dict.get(l, '')}\" for l in letters]\n",
        "    return f\"Question: {question}\\n\" + \"\\n\".join(choices) + \"\\nAnswer:\"\n",
        "\n",
        "def get_prediction(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=5)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
        "    return response.strip()\n",
        "\n",
        "# Load test set\n",
        "df_test = pd.read_json('QA_LLM/dataset/test.json')\n",
        "letter_to_index = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4}\n",
        "\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "for i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
        "    prompt = format_prompt(row[\"question\"], row[\"answers\"])\n",
        "    pred = get_prediction(prompt)\n",
        "\n",
        "    # Prendre uniquement la première lettre présente dans la prédiction\n",
        "    pred_letter = next((c for c in pred if c in letter_to_index), None)\n",
        "    true_letter = row[\"correct_answers\"][0]\n",
        "\n",
        "    is_correct = (pred_letter == true_letter)\n",
        "    correct += is_correct\n",
        "    results.append({\n",
        "        \"question\": row[\"question\"],\n",
        "        \"predicted\": pred_letter,\n",
        "        \"correct\": true_letter,\n",
        "        \"success\": is_correct\n",
        "    })\n",
        "\n",
        "accuracy = correct / len(df_test)\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def format_input(example):\n",
        "    question = example[\"question\"]\n",
        "    answers = example[\"answers\"]\n",
        "    correct = example[\"correct_answers\"][0]  # 'a', 'b', etc.\n",
        "\n",
        "    letters = ['a', 'b', 'c', 'd', 'e']\n",
        "    choices = [f\"{l}) {answers.get(l, '')}\" for l in letters]\n",
        "    prompt = f\"Question: {question}\\n\" + \"\\n\".join(choices) + \"\\nAnswer:\"\n",
        "    return {\n",
        "        \"input_text\": prompt,\n",
        "        \"target_text\": correct\n",
        "    }\n",
        "\n",
        "# Convert DataFrame to Hugging Face dataset\n",
        "train_ds = Dataset.from_pandas(df_train)\n",
        "val_ds = Dataset.from_pandas(df_dev)\n",
        "\n",
        "# Apply formatting\n",
        "train_ds = train_ds.map(format_input)\n",
        "val_ds = val_ds.map(format_input)\n"
      ],
      "metadata": {
        "id": "24b8AyUro_VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    inputs = tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    targets = tokenizer(\n",
        "        example[\"target_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=5\n",
        "    )\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "train_ds = train_ds.map(tokenize, remove_columns=train_ds.column_names)\n",
        "val_ds = val_ds.map(tokenize, remove_columns=val_ds.column_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "mHyP4CCSpDSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoModelForSeq2SeqLM,DataCollatorForSeq2Seq\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" #to disable automatic Weights & Biases logging\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "accuracy_metric = load_metric(\"accuracy\")  # Load accuracy metric from Huggingface datasets\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # Décodage des prédictions et des labels (on ignore les tokens spéciaux)\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Extraction de la première lettre, ou '?' si vide\n",
        "    def first_letter_or_question_mark(text):\n",
        "        text = text.strip().lower()\n",
        "        return text[0] if len(text) > 0 else \"?\"\n",
        "\n",
        "    decoded_preds = [first_letter_or_question_mark(p) for p in decoded_preds]\n",
        "    decoded_labels = [first_letter_or_question_mark(l) for l in decoded_labels]\n",
        "\n",
        "    # Calcul de l'accuracy\n",
        "    accuracy = accuracy_score(decoded_labels, decoded_preds)\n",
        "\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./flan-t5-finetuned-qa\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=True  # si tu as un GPU avec support\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "2AV3cngFpFxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}